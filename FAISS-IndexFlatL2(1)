import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import google.generativeai as genai
import faiss
from langchain.docstore.document import Document
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import json
import requests
import time
import logging
import numpy as np
import spacy
import sys
import fitz  # Import PyMuPDF

# Setup logging - minimal, only errors
logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load API key for Gemini
load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Embedding Function
def get_embedding_from_api(text, request_id=None, retries=0):
    try:
        if not isinstance(text, str):
            return False, None

        text = preprocess_text(text, request_id)

        embedding_endpoint = os.getenv("EMBEDDING_API_ENDPOINT")

        response = requests.post(url=embedding_endpoint, json={"text": text})
        response.raise_for_status()
        result = response.json()
        embedding = result["embedding"][0]

        return True, embedding

    except requests.exceptions.RequestException as e:
        error_log(400, f"API request exception occurred: {str(e)}", "get_embedding", request_id)
        return handle_embedding_error(text, request_id, retries)

    except Exception as e:
        error_log(400, f"Other exception occurred: {str(e)}", "get_embedding", request_id)
        return handle_embedding_error(text, request_id, retries)

def handle_embedding_error(text, request_id, retries):
    retries = retries + 1
    embedding_retries = 3

    if retries <= embedding_retries:
        return get_embedding_from_api(text, request_id, retries=retries)

    return False, None

def error_log(status_code, message, function_name, request_id=None):
    logger.error(f"Request ID: {request_id} - Status Code: {status_code} - Function: {function_name} - {message}")

def preprocess_text(text, request_id=None):
    return text

# --- SpaCy Stop Words Removal ---
nlp = spacy.load("en_core_web_sm")

def remove_stopwords_spacy(text):
    doc = nlp(text)
    filtered_tokens = [token.text for token in doc if not token.is_stop]
    return " ".join(filtered_tokens)

# Extract and Process PDFs using PyMuPDF (fitz)
def get_pdf_text(pdf_docs):
    """Extract text from uploaded PDFs using PyMuPDF (fitz) and return text chunks."""
    text = ""
    for pdf in pdf_docs:
        try:
            with fitz.open(stream=pdf.read(), filetype="pdf") as doc:  # Open PDF using stream
                for page in doc:
                    page_text = page.get_text()
                    cleaned_text = remove_stopwords_spacy(page_text)
                    text += cleaned_text
        except Exception as e:
            st.error(f"Error processing PDF: {e}")
            return [] #Return empty list if error occurs
    return get_text_chunks(text)

# Split Text into Chunks
def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=20)
    return text_splitter.split_text(text)

# --- Modified get_vector_store for persistence ---
def get_vector_store(text_chunks):
    index_file = "./data/faiss_index"
    text_file = "retrieved_text.json"
    embeddings_list = []
    new_stored_texts = []

    if os.path.exists(index_file) and os.path.exists(text_file):
        index = faiss.read_index(index_file)
        with open(text_file, "r") as f:
            stored_texts = json.load(f)
    else:
        dimension = 768
        index = faiss.IndexFlatL2(dimension)
        stored_texts = []
        if not os.path.exists("./data"):
            os.makedirs("./data")

    start_index_build = time.time()
    for chunk in text_chunks:
        success, embedding = get_embedding_from_api(chunk)
        print(f"Embedding success: {success}") #added print
        if success:
            embeddings_list.append(embedding)
            new_stored_texts.append(chunk)
        else:
            embeddings_list.append([0.0] * 768)

    if not embeddings_list:
        st.error("Failed to generate embeddings for any text chunk in this batch.")
        return None, None

    embeddings_array = np.array(embeddings_list).astype("float32")

    index.add(embeddings_array)
    stored_texts.extend(new_stored_texts)

    faiss.write_index(index, index_file)
    with open(text_file, "w") as f:
        json.dump(stored_texts, f)

    end_index_build = time.time()
    index_build_time = end_index_build - start_index_build

    return index, stored_texts, index_build_time

# --- Question Validation and Conversational Chain ---
def validate_question_relevance(question, context):
    model = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.3)
    validation_prompt = f"""
    Here is a document excerpt:
    {context}

    Here is a user question:
    {question}

    If the question is directly related to the document, respond with only "True".
    If the question is not related to the document, respond with only "False".
    """
    response = model.predict(validation_prompt).strip()
    return response == "True"

def get_conversational_chain():
    prompt_template = """
    Answer the question using the provided context if available.
    If the answer is not found in the context, generate a response based on your general knowledge.

    Context:
    {context}

    Question:
    {question}

    Answer:
    """
    model = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.3)
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    return load_qa_chain(model, chain_type="stuff", prompt=prompt)

# --Perform Vector Search in FAISS
def user_input(user_question):
    try:
        index = faiss.read_index("./data/faiss_index")

        with open("retrieved_text.json", "r") as f:
            retrieved_text = json.load(f)

        success, query_embedding = get_embedding_from_api(user_question)
        if not success:
            st.error("Failed to generate embedding for the question.")
            return

        query_embedding = np.array(query_embedding).reshape(1, -1).astype("float32")
        start_search = time.time()
        _, I = index.search(query_embedding, k=5)
        end_search = time.time()
        search_time = end_search - start_search

        docs = [Document(page_content=retrieved_text[i]) for i in I[0] if i < len(retrieved_text)]

        print(f"Query Embedding: {query_embedding}") #added print
        print(f"FAISS Search Indices: {I}") #added print
        print(f"Retrieved Docs: {[doc.page_content for doc in docs]}") #added print

    except Exception as e:
        st.error(f"FAISS Index Error: {str(e)}")
        docs = []
        search_time = 0

    document_context = "\n".join([doc.page_content for doc in docs]) if docs else ""

    if document_context and validate_question_relevance(user_question, document_context):
        chain = get_conversational_chain()
        response = chain({"input_documents": docs, "question": user_question}, return_only_outputs=True)
        answer = response["output_text"]
    else:
        model = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.3)
        answer = model.invoke(user_question)
        answer = dict(answer).values()

    st.write("Reply:", answer)
    st.write(f"Search time: {search_time:.4f} seconds")

def main():
    st.set_page_config(page_title="Chat PDF")
    st.header("Interactive RAG-based LLM for Multi-PDF Document Analysis", divider='rainbow')

    user_question = st.text_input("Ask a Question from the PDF Files or General Knowledge")
    if user_question:
        user_input(user_question)

    with st.sidebar:
        st.title("Menu:")
        pdf_docs = st.file_uploader("Upload your PDF Files and Click on the Submit & Process Button", accept_multiple_files=True)
        if st.button("Submit & Process"):
            if not pdf_docs:
                st.warning("Please upload PDF files before processing.")
            else:
                with st.spinner("Processing..."):
                    text_chunks = get_pdf_text(pdf_docs)
                    if text_chunks:
                        index, stored_texts, index_build_time = get_vector_store(text_chunks)
                        if index:
                            st.success("Processing Complete! You can now ask questions based on all uploaded PDFs.")
                            st.write(f"Index build time: {index_build_time:.4f} seconds")
                            index_size = sys.getsizeof(index)
                            st.write(f"Index size: {index_size} bytes")
                        else:
                            st.error("Vector store creation/update failed for this batch. Existing knowledge base remains.")
                    else:
                        st.error("No text extracted from PDFs in this batch. Please upload valid files.")

if __name__ == "__main__":
    main()
